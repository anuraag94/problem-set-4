---
title: "Problem Set 4"
author: "Pete Cuppernull"
date: "2/16/2020"
output: pdf_document
---
Load packages
```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(rcfss)
library(knitr)
library(splines)
library(lattice)
library(here)
library(patchwork)
library(margins)
set.seed(1414)

```

Load Data
```{r, warning=FALSE, message=FALSE}
gss_test <- read_csv(here("data/gss_test.csv"))
gss_train <- read_csv(here("data/gss_train.csv"))
```

###1. Polynomial Regression
```{r}
egalit_poly <- glm(egalit_scale ~ poly(income06, degree = 2), data = gss_train, family = gaussian) 

holdout_results <- function(splits, i) {
  # Fit the model to the training set
  mod <- glm(egalit_scale ~ poly(income06, i, raw = TRUE), data = analysis(splits))
  
  # Save the heldout observations
  holdout <- assessment(splits)
  
  # `augment` will save the predictions with the holdout data set
  res <- augment(mod, newdata = holdout) %>%
    # calculate the metric
    mse(truth = egalit_scale, estimate = .fitted)
  
  # Return the assessment data set with the additional columns
  res
}

# function to return MSE for a specific fit
poly_mse <- function(i, vfold_data){
  vfold_mod <- vfold_data %>%
    mutate(results = map(splits, holdout_results, i)) %>%
    unnest(results) %>%
    spread(.metric, .estimate)
  
  mean(vfold_mod$mse)
}

# split Auto into 10 folds
gss_cv10 <- vfold_cv(data = gss_train, 
                      v = 10)

cv_mse <- tibble(terms = seq(from = 1, 
                             to = 5),
                 mse_vfold = map_dbl(terms, poly_mse, gss_cv10))
cv_mse
which.min(cv_mse$mse_vfold)

ggplot(cv_mse, aes(terms, mse_vfold)) +
  geom_line() +
  scale_color_brewer(type = "qual") +
  labs(title = "MSE estimates",
       x = "Degree of Polynomial",
       y = "Mean Squared Error",
       color = "CV Method")

##plot the fit to the data

ggplot(gss_train, aes(income06, egalit_scale)) +
  geom_jitter(alpha = .3) +
  stat_smooth(method='lm', formula = y ~ poly(x, degree = 2)) +
  labs(title = "Plot of Best Fit Polynomial",
       subtitle = "Order of 2",
       x = "Income Level",
       y = "Egalitarianism")
```
The optimal degree of the polynomial is 2. 

Average Marginal Effects
```{r}

dY <- diff(gss_train$egalit_scale)/diff(gss_train$income06)
dX <- rowMeans(embed(gss_train$income06,2))
originalData <- data.frame(X = gss_train$income06, Y = gss_train$egalit_scale, type = "Original")
derivativeData <- data.frame(X = dX, Y = dY, type = "Derivative")


derivate_averages <- derivativeData %>%
  group_by(X) %>%
  filter(Y != -Inf) %>%
  filter(Y != Inf) %>%
  summarize(marginal_effect = mean(Y))
  #mutate(x2 = (X*2),
       #  is.whole = if_else(x2 %% 2 == 0, TRUE, FALSE)) %>%
  #filter(is.whole == TRUE)

ggplot() +
  geom_jitter(data = gss_train, mapping = aes(income06, egalit_scale), alpha = .3) +
  stat_smooth(data = gss_train, 
              mapping = aes(income06, egalit_scale), 
              method='lm', 
              formula = y ~ poly(x, degree = 2)) +
  geom_line(data = derivate_averages, mapping = aes(X, marginal_effect), color = "red") +
  labs(title = "Average Marginal Effect of Income on Egalitarianism",
       subtitle = "Marginal Effect indicated by red line",
       x = "Income Level",
       y = "Egalitarian Scale")
  
```
The average marginal effect of income level on egalitarianism remains largely negative throughout the range of income levels. This indicates that as individuals make more money, they are less likely to have egalitarian preferences. We can observe, however, that from income levels 6-8, egalitarianism increases as income increases -- this would suggest a possible anomaly for this income range where individuals become more egalitarian as their income increases.





###2. Step Function
```{r}
egalit_step <- glm(egalit_scale ~ cut(income06, 5), data = gss_train) 


holdout_results_step <- function(splits, i) {
  # Fit the model to the training set
  mod <- glm(egalit_scale ~ cut(income06, i), 
             data = analysis(splits))
  
  # Save the heldout observations
  holdout <- assessment(splits)
  
  # `augment` will save the predictions with the holdout data set
  res <- augment(mod, newdata = holdout) %>%
    # calculate the metric
    mse(truth = egalit_scale, estimate = .fitted)
  
  # Return the assessment data set with the additional columns
  res
}

# function to return MSE for a specific fit
step_mse <- function(i, vfold_data){
  vfold_mod <- vfold_data %>%
    mutate(results = map(splits, holdout_results_step, i)) %>%
    unnest(results) %>%
    spread(.metric, .estimate)
  
  mean(vfold_mod$mse)
}

# split Auto into 10 folds
gss_cv10_step <- vfold_cv(data = gss_train, 
                      v = 10)

cv_mse <- tibble(terms = seq(from = 2, 
                             to = 15),
                 mse_vfold = map_dbl(terms, step_mse, gss_cv10_step))
cv_mse %>%
  ggplot(aes(terms, mse_vfold)) +
  geom_line() +
  labs(Title = "MSEs of Step Functions with Varying Cuts",
       y = "MSE",
       x = "Number of Cuts")


```
The optimum number of cuts basic on the cross-validation MSEs is 4.


###3. Natural Regression Spline
```{r, message=FALSE}

gss_spline <- function(splits, df = NULL){
  # estimate the model on each fold
  model <- glm(egalit_scale ~ ns(income06, df = df),
               data = analysis(splits))

  
  model_acc <- augment(model, newdata = assessment(splits)) %>%
    accuracy(truth = factor(egalit_scale, levels = 1:35), estimate = factor(round(.fitted), levels = 1:35))
  
  mean(model_acc$.estimate)
}



tune_over_knots <- function(splits, knots){
  gss_spline(splits, df = knots + 3)
}

# estimate CV error for knots in 0:25
results <- vfold_cv(gss_train, v = 10)


expand(results, id, knots = 1:8) %>%
  left_join(results) %>%
  mutate(acc = map2_dbl(splits, knots, tune_over_knots)) %>%
  group_by(knots) %>%
  summarize(acc = mean(acc)) %>%
  mutate(err = 1 - acc) %>%
  ggplot(aes(knots, err)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Optimal number of knots for natural cubic spline regression",
       x = "Knots",
       y = "10-fold CV error")

best_model <- glm(egalit_scale ~ ns(income06, df = 10),
               data = gss_train)
summary(best_model)
```
The optimum number of knots is 7. 
